# -*- coding: utf-8 -*-
from DrissionPage import ChromiumPage, ChromiumOptions
import pandas as pd
import time
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime
from openpyxl import load_workbook, Workbook
from urllib.parse import urlencode, quote
import sys
import shutil
import math

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
API_URL_BASE = "https://shandong.chinatax.gov.cn/module/web/jpage/dataproxy.jsp"
HOME_URL = "https://shandong.chinatax.gov.cn/col/col1053/index.html?number=A0301"
BASE_URL = "https://shandong.chinatax.gov.cn"

COLUMN_ID = 1053
UNIT_ID = 48166

# æ–‡ä»¶å
FILE_NAME = "å±±ä¸œç¨åŠ¡_å…¨é‡æ•°æ®.xlsx"
VERSION = "v21.0 (ç›´æ¥å¯¼èˆª + åŒé‡åˆ†é¡µå‚æ•°)"


# ================= ğŸ“‚ è‡ªåŠ¨åŒ–æ–‡ä»¶ç®¡ç† =================

def get_desktop_path():
    """ç›´æ¥è·å–æ¡Œé¢è·¯å¾„"""
    return os.path.join(os.path.expanduser("~"), "Desktop", FILE_NAME)


def init_or_check_excel(filepath):
    print(f"\nğŸ“‚ ç›®æ ‡æ–‡ä»¶: {filepath}")

    if os.path.exists(filepath):
        print("âœ… [æ£€æµ‹ç»“æœ] æ–‡ä»¶å·²å­˜åœ¨ï¼")
        print("   -> æ¨¡å¼ï¼šã€æ–­ç‚¹ç»­ä¼ ã€‘(è‡ªåŠ¨è·³è¿‡æ—§æ•°æ®)")
        # è‡ªåŠ¨å¤‡ä»½
        try:
            bak_path = filepath + ".bak"
            shutil.copy(filepath, bak_path)
        except:
            pass
    else:
        print("ğŸ†• [æ£€æµ‹ç»“æœ] æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
        print("   -> æ¨¡å¼ï¼šã€å…¨æ–°æŠ“å–ã€‘")
        wb = Workbook()
        ws = wb.active
        ws.append(["æ ‡é¢˜", "å‘æ–‡æœºæ„", "å‘æ–‡å­—å·", "å‘æ–‡æ—¥æœŸ", "æœ‰æ•ˆæ€§", "æ˜¯å¦æ¶‰ç¨æ³•å¾‹", "æ­£æ–‡å†…å®¹", "é“¾æ¥"])
        wb.save(filepath)


def get_history_links(filepath):
    """è¯»å–å†å²é“¾æ¥"""
    if not os.path.exists(filepath): return set()
    try:
        df = pd.read_excel(filepath, engine="openpyxl", usecols=["é“¾æ¥"])
        return set(df["é“¾æ¥"].dropna().astype(str).tolist())
    except:
        return set()


def save_row_immediately(row_data, filepath):
    """å®æ—¶å†™å…¥"""
    try:
        wb = load_workbook(filepath)
        ws = wb.active
        ws.append(list(row_data.values()))
        wb.save(filepath)
        print(".", end="", flush=True)
    except PermissionError:
        print(f"\nğŸš¨ [ä¸¥é‡] æ–‡ä»¶è¢«å ç”¨ï¼è¯·å…³é—­æ¡Œé¢çš„ Excel æ–‡ä»¶ï¼")
    except Exception as e:
        print(f"\nâŒ å†™å…¥å¤±è´¥: {e}")


# ================= ğŸ§  æå–é€»è¾‘ =================
def safe_re_extract(pattern, text):
    try:
        m = re.search(pattern, text, re.DOTALL)
        if m: return m.group(1).strip()
    except:
        pass
    return ""


def extract_detail(page, url):
    try:
        # è®¿é—®è¯¦æƒ…é¡µ
        page.get(url, timeout=10)

        # é‡åˆ°é˜²ç«å¢™ç­‰å¾…
        if "å®‰å…¨æ£€æŸ¥" in page.title:
            print(f"\nâš ï¸ é­é‡é˜²ç«å¢™: {url}")
            time.sleep(5)

        html = page.html
        soup = BeautifulSoup(html, 'html.parser')

        info = {
            "æ ‡é¢˜": "", "å‘æ–‡æœºæ„": "", "å‘æ–‡å­—å·": "",
            "å‘æ–‡æ—¥æœŸ": "", "æœ‰æ•ˆæ€§": "æœªæ³¨æ˜",
            "æ˜¯å¦æ¶‰ç¨æ³•å¾‹": "æœªæ³¨æ˜",
            "æ­£æ–‡å†…å®¹": "", "é“¾æ¥": url
        }

        # 1. æºç æå– (æœ€ç¨³)
        if not info['å‘æ–‡æœºæ„']:
            for tag in ['å‘æ–‡æœºå…³', 'å‘å¸ƒæœºæ„', 'å‘æ–‡å•ä½']:
                val = safe_re_extract(f'(.*?)', html)
                if val:
                    info['å‘æ–‡æœºæ„'] = val
                    break
        if not info['å‘æ–‡å­—å·']:
            info['å‘æ–‡å­—å·'] = safe_re_extract(r'(.*?)', html)
        if not info['å‘æ–‡æ—¥æœŸ']:
            for tag in ['å‘æ–‡æ—¥æœŸ', 'å‘å¸ƒæ—¥æœŸ', 'æˆæ–‡æ—¥æœŸ']:
                val = safe_re_extract(f'(.*?)', html)
                if val:
                    info['å‘æ–‡æ—¥æœŸ'] = val
                    break
        if not info['æ ‡é¢˜']:
            info['æ ‡é¢˜'] = safe_re_extract(r'(.*?)', html)

        # 2. è¡¨æ ¼è¡¥æ•‘
        try:
            meta_table = soup.find('table', id='xxgkbg')
            if meta_table:
                tds = meta_table.find_all('td')
                for i, td in enumerate(tds):
                    txt = td.get_text(strip=True)
                    if not info['å‘æ–‡æœºæ„'] and ('å‘æ–‡æœºå…³' in txt or 'å‘å¸ƒæœºæ„' in txt) and i + 1 < len(tds):
                        info['å‘æ–‡æœºæ„'] = tds[i + 1].get_text(strip=True)
                    if not info['å‘æ–‡å­—å·'] and 'å‘æ–‡å­—å·' in txt and i + 1 < len(tds):
                        info['å‘æ–‡å­—å·'] = tds[i + 1].get_text(strip=True)
                    if not info['å‘æ–‡æ—¥æœŸ'] and ('æ—¥æœŸ' in txt) and i + 1 < len(tds):
                        info['å‘æ–‡æ—¥æœŸ'] = tds[i + 1].get_text(strip=True)
                    if not info['æœ‰æ•ˆæ€§'] and 'æœ‰æ•ˆæ€§' in txt and i + 1 < len(tds):
                        info['æœ‰æ•ˆæ€§'] = tds[i + 1].get_text(strip=True)
                    if 'æ˜¯å¦æ¶‰ç¨æ³•å¾‹' in txt and i + 1 < len(tds):
                        info['æ˜¯å¦æ¶‰ç¨æ³•å¾‹'] = tds[i + 1].get_text(strip=True)
        except:
            pass

        # 3. æ–‡æœ¬è¡¥æ•‘
        if not info['å‘æ–‡æ—¥æœŸ']:
            main_div = soup.find('div', class_='main_content')
            if main_div:
                val = safe_re_extract(r"æ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})", main_div.get_text())
                if val: info['å‘æ–‡æ—¥æœŸ'] = val
        if info['æœ‰æ•ˆæ€§'] == "æœªæ³¨æ˜":
            main_div = soup.find('div', class_='main_content')
            if main_div:
                val = safe_re_extract(r"æœ‰æ•ˆæ€§[ï¼š:]\s*(.*?)(?:\s|$)", main_div.get_text())
                if val: info['æœ‰æ•ˆæ€§'] = val
        if not info['æ ‡é¢˜']:
            t = soup.find('meta', attrs={'name': 'ArticleTitle'})
            if t: info['æ ‡é¢˜'] = t.get('content', '')

        # æ­£æ–‡
        content_div = soup.find(id='zoom') or soup.find(class_='TRS_Editor')
        if content_div:
            info['æ­£æ–‡å†…å®¹'] = content_div.get_text(strip=True)[:30000]
        else:
            div3 = soup.find('div', class_='main_content3')
            if div3: info['æ­£æ–‡å†…å®¹'] = div3.get_text(strip=True)[:30000]

        print(f"  [ok] {info['æ ‡é¢˜'][:10]}... | æ¶‰ç¨:{info['æ˜¯å¦æ¶‰ç¨æ³•å¾‹']}")
        return info

    except Exception as e:
        print(f"\n    âŒ è¯¦æƒ…é”™è¯¯: {e}")
        return None


# ================= ğŸš€ ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨é‡‡é›†å™¨ - {VERSION}")

    # 1. è‡ªåŠ¨è·å–æ¡Œé¢è·¯å¾„
    save_path = get_desktop_path()

    # 2. åˆå§‹åŒ–æ£€æŸ¥
    init_or_check_excel(save_path)

    # 3. è¯»å–æ–­ç‚¹
    processed_urls = get_history_links(save_path)
    print(f"ğŸ“š å†å²è®°å½•: {len(processed_urls)} æ¡ (å°†è‡ªåŠ¨è·³è¿‡)")

    # 4. æµè§ˆå™¨
    co = ChromiumOptions()
    co.set_user_agent(
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    co.set_argument('--blink-settings=imagesEnabled=false')
    co.ignore_certificate_errors()
    page = ChromiumPage(addr_or_opts=co)

    print(f"ğŸŒ åˆå§‹åŒ–: {HOME_URL}")
    page.get(HOME_URL)
    time.sleep(2)

    BATCH_SIZE = 45

    # æŠ“å–å¾ªç¯ (ä»ç¬¬1æ¡åˆ°ç¬¬3000æ¡)
    for start_rec in range(1, 3000, BATCH_SIZE):
        end_rec = start_rec + BATCH_SIZE - 1

        # ã€æ ¸å¿ƒä¿®å¤ã€‘è®¡ç®—é¡µç ï¼šTRSç³»ç»Ÿæœ‰æ—¶ä¾èµ–pageå‚æ•°
        # start=1 -> page=1, start=46 -> page=2
        page_num = math.ceil(start_rec / BATCH_SIZE)

        print(f"\nğŸ”„ è¯·æ±‚åŒºé—´: {start_rec} - {end_rec} (ç¬¬ {page_num} é¡µ)")

        # ã€æ ¸å¿ƒä¿®å¤ã€‘æ„é€ å®Œæ•´çš„ URLï¼Œç›´æ¥è®©æµè§ˆå™¨è·³è½¬è¿‡å»ï¼
        # åŒ…å«äº†æ‰€æœ‰å¯èƒ½çš„å‚æ•°ï¼Œç¡®ä¿åˆ†é¡µç”Ÿæ•ˆ
        params = {
            "col": "1",
            "appid": "1",
            "webid": "1",
            "path": "/",
            "columnid": str(COLUMN_ID),
            "unitid": str(UNIT_ID),
            "webname": "å›½å®¶ç¨åŠ¡æ€»å±€å±±ä¸œçœç¨åŠ¡å±€",
            "permissiontype": "0",
            "page": str(page_num),  # æ˜¾å¼æŒ‡å®šé¡µç 
            "startrecord": str(start_rec),  # æ˜¾å¼æŒ‡å®šèµ·å§‹è¡Œ
            "endrecord": str(end_rec)  # æ˜¾å¼æŒ‡å®šç»“æŸè¡Œ
        }

        full_api_url = f"{API_URL_BASE}?{urlencode(params)}"

        # è®©æµè§ˆå™¨ç›´æ¥è®¿é—® XML æ¥å£
        page.get(full_api_url)
        xml_text = page.html  # è·å–é¡µé¢å†…å®¹

        if not xml_text or "wzws" in xml_text:
            print("âš ï¸ é˜²ç«å¢™æ‹¦æˆªï¼Œæš‚åœ5ç§’...")
            time.sleep(5)
            continue

        pattern = r'<record><!\[CDATA\[(.*?)\]\]></record>'
        matches = re.findall(pattern, xml_text, re.DOTALL)

        if not matches:
            print(f"ğŸ æœ¬é¡µæ— æ•°æ® (æŠ“å–ç»“æŸ)ã€‚")
            break

        print(f"   ğŸ“„ å‘ç° {len(matches)} æ¡")

        # æ£€æŸ¥æ˜¯å¦ä¾ç„¶è¿”å› 300 æ¡
        if len(matches) > BATCH_SIZE + 20:
            print("âš ï¸ ä¸¥é‡è­¦å‘Šï¼šæœåŠ¡å™¨ä¾ç„¶è¿”å›å…¨éƒ¨æ•°æ®ï¼ˆåˆ†é¡µå½»åº•å¤±æ•ˆï¼‰ã€‚")
            print("   -> æ­£åœ¨å¯åŠ¨ã€å¼ºåˆ¶è·³è¿‡ã€‘æ¨¡å¼ï¼Œç›´åˆ°æ‰¾åˆ°æ–°æ•°æ®ä¸ºæ­¢...")

        new_count = 0
        for html_snippet in matches:
            soup = BeautifulSoup(html_snippet, 'html.parser')
            link_tag = soup.find('a')
            if not link_tag: continue

            title = link_tag.get_text(strip=True)
            href = link_tag.get('href')
            full_url = BASE_URL + href if href.startswith('/') else href

            # æ–­ç‚¹è·³è¿‡
            if full_url in processed_urls:
                continue

            print(f"   Downloading: {title[:15]}...", end="")

            detail_data = extract_detail(page, full_url)

            if detail_data:
                if not detail_data['æ ‡é¢˜']: detail_data['æ ‡é¢˜'] = title
                save_row_immediately(detail_data, save_path)
                processed_urls.add(full_url)
                new_count += 1

            # æŠ“å®Œè¯¦æƒ…é¡µåï¼Œä¼‘æ¯ä¸€ä¸‹
            time.sleep(0.1)

        if len(matches) > 0 and new_count == 0:
            print("   (æœ¬é¡µæ•°æ®å·²å…¨éƒ¨å­˜åœ¨ï¼Œè·³è¿‡)")
        elif new_count > 0:
            print(f"   (æœ¬é¡µæ–°å¢å…¥åº“ {new_count} æ¡)")

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {save_path}")


if __name__ == "__main__":
    main()